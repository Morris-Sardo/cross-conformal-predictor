{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "def normalation(X_train, X_test, y_train, y_test):\n",
    "#define scaler\n",
    "    scaler ={\n",
    "        \"StandardScaler\": StandardScaler(),\n",
    "        \"MinMaxScaler\": MinMaxScaler(),\n",
    "        \"RobustScaler\":RobustScaler(),\n",
    "        \"Normalizer\": Normalizer()\n",
    "    }\n",
    "\n",
    "    param_grid = {'svc__C':[0.01,0.1,1,10,100], 'svc__gamma':[0.001,0.01,0.1,1]}\n",
    "\n",
    "\n",
    "    test_error_rate = 10\n",
    "    #loop through each scaler, create a piperline. \n",
    "    for scaler_name, scaler in scaler.items():\n",
    "        pipe = Pipeline([(\"scaler\", scaler),(\"svc\",SVC())])\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid,cv=5,n_jobs=2)\n",
    "        #grid.fit(X_train_Wine, y_train_Wine)\n",
    "        grid.fit(X_train,y_train)\n",
    "        model = grid.best_estimator_\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=2404)\n",
    "        p_values = []\n",
    "\n",
    "         # Conformity computation for cross-conformal prediction\n",
    "        for train_index, calibration_index in kf.split(X_train):\n",
    "            # Split into training and calibration sets\n",
    "            X_train_cv, X_calib = X_train[train_index], X_train[calibration_index]\n",
    "            y_train_cv, y_calib = y_train[train_index], y_train[calibration_index]\n",
    "\n",
    "\n",
    "            # Fit model on training fold\n",
    "            model.fit(X_train_cv, y_train_cv)\n",
    "    \n",
    "            # Compute conformity scores using decision_function\n",
    "            calib_scores = model.decision_function(X_calib)\n",
    "            calib_pred = np.max(calib_scores, axis=1)\n",
    "    \n",
    "            # Evaluate on test set\n",
    "            test_scores = model.decision_function(X_test)\n",
    "            test_pred = np.max(test_scores, axis=1)\n",
    "    \n",
    "            # Compute p-values\n",
    "            for i, score in enumerate(test_pred):\n",
    "                p_value = np.mean(calib_pred <= score)\n",
    "                p_values.append(p_value)\n",
    "\n",
    "        #Convert p-values to numpy array\n",
    "        p_values = np.array(p_values).reshape(-1, len(X_test))\n",
    "        #p_values = np.array(p_values)\n",
    "\n",
    "        # Compute the calibration curve\n",
    "        eps = np.linspace(0, 1, 100)\n",
    "        error_rate = []\n",
    "\n",
    "        for e in eps:\n",
    "            error_rate.append(np.mean(p_values < e))\n",
    "\n",
    "        # Plot calibration curve\n",
    "        plt.plot(eps, error_rate, label='Calibration Curve')\n",
    "        plt.plot(eps, eps, '--', label='Ideal')\n",
    "        plt.xlabel('Significance Level (Îµ)')\n",
    "        plt.ylabel('Error Rate')\n",
    "        plt.title('Calibration Curve')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        #print( y_test)\n",
    "        #print(model.predict(X_test))\n",
    "        #p_values = np.array(p_values).reshape(-1, len(X_test))\n",
    "        average_false_p_value = np.mean(p_values[:, y_test != model.predict(X_test)])\n",
    "        # Compute the average false p-value\n",
    "        #average_false_p_value = np.mean(p_values[y_test != model.predict(X_test)])\n",
    "        print(f\"Average False p-value on Test Set: {average_false_p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2677147537.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    def (X_train, X_test, y_train, y_test):\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#question 7b\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def mormalization11(X_train, X_test, y_train, y_test):\n",
    "    # Define scalers\n",
    "    scalers = {\n",
    "        \"StandardScaler\": StandardScaler(),\n",
    "        \"MinMaxScaler\": MinMaxScaler(),\n",
    "        \"RobustScaler\": RobustScaler(),\n",
    "        \"Normalizer\": Normalizer()\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        pipe = Pipeline([(\"scaler\", scaler), (\"svc\", SVC(decision_function_shape='ovr'))])\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=2)\n",
    "        #grid.fit(X_train, y_train)\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=2404)\n",
    "        # Conformity computation for cross-conformal prediction\n",
    "        for train_index, calibration_index in kf.split(X_train):\n",
    "            # Split into training and calibration sets\n",
    "            X_rest, X_fold = X_train[train_index], X_train[calibration_index]\n",
    "            y_rest, y_fold = y_train[train_index], y_train[calibration_index]\n",
    "            grid.fit(X_rest, y_rest)\n",
    "            cal = grid.decision_function(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalation11' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnormalation11\u001b[49m(X_train_Wine,X_test_Wine,y_train_Wine,y_test_Wine)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalation11' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def conformal_score(X_train, y_train, X_calibrate, y_calibrate, new_instance, model):\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get decision function scores for the calibration set and new instance\n",
    "    calibration_scores = model.decision_function(X_calibrate)\n",
    "    new_instance_score = model.decision_function([new_instance])\n",
    "    \n",
    "    # Combine and rank scores\n",
    "    all_scores = np.concatenate((calibration_scores, new_instance_score))\n",
    "    ranks = rankdata(-all_scores, method='ordinal')  # We use 'negative' for higher scores to have higher ranks\n",
    "    \n",
    "    # Compute p-value for the new instance (last element in ranks array)\n",
    "    p_value = (np.sum(ranks[-1] <= ranks[:-1]) + 1) / (len(all_scores) + 1)\n",
    "    return p_value\n",
    "\n",
    "# Load data\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Prepare K-fold\n",
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "# SVM model\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Select new instance\n",
    "new_instance = X[100]  # Example instance\n",
    "\n",
    "# Perform cross-conformal prediction\n",
    "p_values = []\n",
    "for train_index, calibrate_index in kf.split(X):\n",
    "    X_train, X_calibrate = X[train_index], X[calibrate_index]\n",
    "    y_train, y_calibrate = y[train_index], y[calibrate_index]\n",
    "    \n",
    "    # Ensure the new instance is not in the calibration set\n",
    "    if new_instance in X_calibrate:\n",
    "        continue\n",
    "    \n",
    "    p_value = conformal_score(X_train, y_train, X_calibrate, y_calibrate, new_instance, svm)\n",
    "    p_values.append(p_value)\n",
    "\n",
    "# Average p-values across all splits where the new instance was not used for calibration\n",
    "average_p_value = np.mean(p_values)\n",
    "print(\"Average p-value for the new instance:\", average_p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i dont know it my the code just upload calculate properly the p-value\n",
    "\n",
    "#question 7 \n",
    "#this is my attemp to implement question 7. I nkeo that the implemantion is not right but i tried anyway.\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def kFolds(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#define scaler\n",
    "    scaler ={\n",
    "        \"StandardScaler\": StandardScaler(),\n",
    "        \"MinMaxScaler\": MinMaxScaler(),\n",
    "        \"RobustScaler\":RobustScaler(),\n",
    "        \"Normalizer\": Normalizer()\n",
    "    }\n",
    "\n",
    "    param_grid = {'svc__C':[0.01,0.1,1,10,100], 'svc__gamma':[0.001,0.01,0.1,1]}\n",
    "\n",
    "\n",
    "    test_error_rate = 10\n",
    "    #loop through each scaler, create a piperline. \n",
    "    for scaler_name, scaler in scaler.items():\n",
    "        pipe = Pipeline([(\"scaler\", scaler),(\"svc\",SVC())])\n",
    "        #for i in range(5,11):\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid,cv=5,n_jobs=2)\n",
    "        #grid.fit(X_train_Wine, y_train_Wine)\n",
    "        grid.fit(X_train,y_train)\n",
    "        #print(\"best parameters: \", grid.best_params_)\n",
    "        model = grid.best_estimator_\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=2404)\n",
    "        p_values = []\n",
    "\n",
    "        # Conformity computation for cross-conformal prediction\n",
    "        for train_index, calibration_index in kf.split(X_train):\n",
    "            # Split into training and calibration sets\n",
    "            X_train_cv, X_calib = X_train[train_index], X_train[calibration_index]\n",
    "            y_train_cv, y_calib = y_train[train_index], y_train[calibration_index]\n",
    "\n",
    "\n",
    "            # Fit model on training fold\n",
    "            model.fit(X_train_cv, y_train_cv)\n",
    "    \n",
    "            # Compute conformity scores  for each fold using decision_function\n",
    "            conformity_scores = model.decision_function(X_calib)\n",
    "\n",
    "            calib_pred = np.max(conformity_scores, axis=1)\n",
    "            #print(\"calibration \",conformity_scores)\n",
    "            #print(calib_pred)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_scores = model.decision_function(X_test)\n",
    "\n",
    "            test_pred = np.max(test_scores, axis=1)\n",
    "            #print(\"test_predict\", test_scores)\n",
    "            for i in range(len(test_pred)):\n",
    "                p_value = np.mean(calib_pred <= test_pred[i])\n",
    "                p_values.append(p_value)\n",
    "\n",
    "            # Compute p-values\n",
    "            # for i, score in enumerate(test_pred):\n",
    "\n",
    "            #     p_value = np.mean(calib_pred <= score)\n",
    "            #     p_values.append(p_value)\n",
    "        #Convert p-values to numpy array\n",
    "        p_values = np.array(p_values).reshape(-1, len(X_test))\n",
    "        #p_values = np.array(p_values)\n",
    "\n",
    "\n",
    "        # Compute the calibration curve\n",
    "        eps = np.linspace(0, 1, 100)\n",
    "        error_rate = []\n",
    "        \n",
    "        for e in eps:\n",
    "            #print(error_rate)\n",
    "            error_rate.append(np.mean(p_values <= e))\n",
    "\n",
    "        # Plot calibration curve\n",
    "        plt.plot(eps, error_rate, label=f'{scaler_name}')\n",
    "        plt.plot(eps, error_rate, label='Calibration Curve')\n",
    "        plt.plot(eps, eps, '--', label='Ideal')\n",
    "        plt.xlabel('Significance Level (Îµ)')\n",
    "        plt.ylabel('Error Rate')\n",
    "        plt.title('Calibration Curve')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        #print( y_test)\n",
    "        #print(model.predict(X_test))\n",
    "        #p_values = np.array(p_values).reshape(-1, len(X_test))\n",
    "        average_false_p_value = np.mean(p_values[:, y_test != model.predict(X_test)])\n",
    "        # Compute the average false p-value\n",
    "        #average_false_p_value = np.mean(p_values[y_test != model.predict(X_test)])\n",
    "        print(f\"Average False p-value on Test Set for : {scaler_name} {average_false_p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 7 \n",
    "#this is my attemp to implement question 7. I nkeo that the implemantion is not right but i tried anyway.\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def kFolds(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#define scaler\n",
    "    scaler ={\n",
    "        \"StandardScaler\": StandardScaler(),\n",
    "        \"MinMaxScaler\": MinMaxScaler(),\n",
    "        \"RobustScaler\":RobustScaler(),\n",
    "        \"Normalizer\": Normalizer()\n",
    "    }\n",
    "\n",
    "    param_grid = {'svc__C':[0.01,0.1,1,10,100], 'svc__gamma':[0.001,0.01,0.1,1]}\n",
    "\n",
    "    count = 0\n",
    "    #test_error_rate = 10\n",
    "    #loop through each scaler, create a piperline. \n",
    "    for scaler_name, scaler in scaler.items():\n",
    "        pipe = Pipeline([(\"scaler\", scaler),(\"svc\",SVC())])\n",
    "        #for i in range(5,11):\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid,cv=5,n_jobs=2)\n",
    "        #grid.fit(X_train_Wine, y_train_Wine)\n",
    "        grid.fit(X_train,y_train)\n",
    "        #print(\"best parameters: \", grid.best_params_)\n",
    "        model = grid.best_estimator_\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=2904)\n",
    "        p_values = []\n",
    "        p_valuesrr = []\n",
    "\n",
    "        # Conformity computation for cross-conformal prediction\n",
    "        for rest_fold_index, fold_index in kf.split(X_train):\n",
    "            # Split into training and calibration sets\n",
    "            X_rest_fold, X_fold = X_train[rest_fold_index], X_train[fold_index]\n",
    "            y_rest, y_fold = y_train[rest_fold_index], y_train[fold_index]\n",
    "\n",
    "\n",
    "            # Fit model on training fold\n",
    "            model.fit(X_rest_fold, y_rest)\n",
    "    \n",
    "            # Compute conformity scores  for each fold using decision_function\n",
    "            conformity_scores = model.decision_function(X_fold)\n",
    "\n",
    "            #retrieve max volues of of calibration set.\n",
    "            calib_pred = np.max(conformity_scores, axis=1)\n",
    "            #print(\"calibration \",conformity_scores)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_scores = model.decision_function(X_test)\n",
    "\n",
    "            #retrieve max volues of of calibration set.\n",
    "            test_pred = np.max(test_scores, axis=1)\n",
    "\n",
    "\n",
    "            #combine all ranks score\n",
    "            ######\n",
    "            #this fucntion user rank calcualtion\n",
    "            #############\n",
    "            for i in range(len(test_pred)):\n",
    "                #print(\"test_pred\",test_pred)\n",
    "\n",
    "                ###############################\n",
    "                #CHECK THE RANK\n",
    "                #############################\n",
    "                rank = np.sum(calib_pred <= test_pred[i]) #rank for each labels.\n",
    "                #print(\"cont \",rank)\n",
    "                #if i == len(test_pred):\n",
    "                \n",
    "                p_valueFUNCTION = ((rank) + 1) /(len(test_pred) + 1)\n",
    "                p_valuesrr.append(p_valueFUNCTION)\n",
    "            #print(\"p_valuerss\", p_valuesrr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            #originaL CODE   \n",
    "            for i in range(len(test_pred)):\n",
    "                #if i == len(test_pred):\n",
    "                p_value = np.mean(calib_pred <= test_pred[i])\n",
    "                p_values.append(p_value)\n",
    "            #print(\"p_values \",len(p_values))\n",
    "\n",
    "        p_values = np.array(p_values).reshape(-1, len(X_test))\n",
    "        p_values_FUNCTION = np.array(p_valuesrr).reshape(-1, len(X_test))\n",
    "        #print(\"p_values \",p_values)\n",
    "        #print(\"p_valuerss\", p_valuesSSSSSSSSS)\n",
    "        #p_values = np.array(p_values)\n",
    "\n",
    "\n",
    "        # Compute the calibration curve\n",
    "        eps = np.linspace(0, 1, 100)\n",
    "        error_rate = []\n",
    "        \n",
    "        for e in eps:\n",
    "            #print(error_rate)\n",
    "            error_rate.append(np.mean(p_values_FUNCTION <= e))\n",
    "\n",
    "        # Plot calibration curve\n",
    "        plt.plot(eps, error_rate, label=f'{scaler_name}')\n",
    "        plt.plot(eps, error_rate, label='Calibration Curve')\n",
    "        plt.plot(eps, eps, '--', label='Ideal')\n",
    "        plt.xlabel('Significance Level (Îµ)')\n",
    "        plt.ylabel('Error Rate')\n",
    "        plt.title('Calibration Curve')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        ##########################################################################\n",
    "        average_false_p_valueN = calculate_average_false_p_value(model, X_test, y_test, p_values)\n",
    "        print(f\"Average False p-valueN on Test Set for : {scaler_name} {average_false_p_valueN}\")\n",
    "\n",
    "\n",
    "        #################################################################################\n",
    "        \n",
    "        average_false_p_value = np.mean(p_values[:, y_test != model.predict(X_test)])\n",
    "        # Compute the average false p-value\n",
    "        #average_false_p_value = np.mean(p_values[y_test != model.predict(X_test)])\n",
    "        print(f\"Average False p-value on Test Set for : {scaler_name} {average_false_p_value}\")\n",
    "        #print(f\"Average False p-value on Test Set for : {scaler_name} {MY_AVERAGE_FALSE_P_VALUE}\")\n",
    "\n",
    "\n",
    "#function responsibe to calcualte the average false p-value.\n",
    "def calculate_average_false_p_value(model, X_test, y_test, p_values):\n",
    "    print(model)\n",
    "    predictions = model.predict(X_test)\n",
    "    incorrect_predictions = y_test != predictions\n",
    "    \n",
    "    p_values = np.array(p_values)\n",
    "    if len(p_values.shape) == 2:\n",
    "        p_values = np.mean(p_values, axis=0)\n",
    "    \n",
    "    false_p_values = p_values[incorrect_predictions]\n",
    "    if false_p_values.size > 0:\n",
    "        average_false_p_value = np.mean(false_p_values)\n",
    "    else:\n",
    "        average_false_p_value = np.nan  # or handle as appropriate\n",
    "\n",
    "    return average_false_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "#question 7 \n",
    "#this is my attemp to implement question 7. I nkeo that the implemantion is not right but i tried anyway.\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def kFolds(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#define scaler\n",
    "    scaler ={\n",
    "        \"StandardScaler\": StandardScaler(),\n",
    "        \"MinMaxScaler\": MinMaxScaler(),\n",
    "        \"RobustScaler\":RobustScaler(),\n",
    "        \"Normalizer\": Normalizer()\n",
    "    }\n",
    "\n",
    "    param_grid = {'svc__C':[0.01,0.1,1,10,100], 'svc__gamma':[0.001,0.01,0.1,1]}\n",
    "\n",
    "    count = 0\n",
    "    #test_error_rate = 10\n",
    "    #loop through each scaler, create a piperline. \n",
    "    for scaler_name, scaler in scaler.items():\n",
    "        pipe = Pipeline([(\"scaler\", scaler),(\"svc\",SVC())])\n",
    "        #for i in range(5,11):\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid,cv=5,n_jobs=2)\n",
    "        #grid.fit(X_train_Wine, y_train_Wine)\n",
    "        grid.fit(X_train,y_train)\n",
    "        #print(\"best parameters: \", grid.best_params_)\n",
    "        model = grid.best_estimator_\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=2404)\n",
    "        p_values = []\n",
    "        p_valuesrr = []\n",
    "\n",
    "        # Conformity computation for cross-conformal prediction\n",
    "        for rest_fold_index, fold_index in kf.split(X_train):\n",
    "            # Split into training and calibration sets\n",
    "            X_rest_fold, X_fold = X_train[rest_fold_index], X_train[fold_index]\n",
    "            y_rest, y_fold = y_train[rest_fold_index], y_train[fold_index]\n",
    "\n",
    "\n",
    "            # Fit model on training fold\n",
    "            model.fit(X_rest_fold, y_rest)\n",
    "    \n",
    "            # Compute conformity scores  for each fold using decision_function\n",
    "            conformity_scores = model.decision_function(X_fold)\n",
    "\n",
    "            #retrieve max volues of of calibration set.\n",
    "            calib_pred = np.max(conformity_scores, axis=1)\n",
    "            #print(\"calibration \",conformity_scores)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_scores = model.decision_function(X_test)\n",
    "\n",
    "            #retrieve max volues of of calibration set.\n",
    "            test_pred = np.max(test_scores, axis=1)\n",
    "            #print(test_pred)\n",
    "            \n",
    "            #originaL CODE   \n",
    "            for i in range(len(test_pred)):\n",
    "                #if i == len(test_pred):\n",
    "                p_value = np.mean(calib_pred <= test_pred[i]) #rank for each labels.\n",
    "                p_values.append(p_value)\n",
    "            #print(\"p_values \",p_values)\n",
    "\n",
    "        p_values = np.array(p_values).reshape(-1, len(X_test))\n",
    "        #p_values_FUNCTION = np.array(p_valuesrr).reshape(-1, len(X_test))\n",
    "        #print(\"p_values \",p_values)\n",
    "        #print(\"p_valuerss\", p_valuesSSSSSSSSS)\n",
    "        #p_values = np.array(p_values)\n",
    "\n",
    "\n",
    "        # Compute the calibration curve\n",
    "        eps = np.linspace(0, 1, 100)\n",
    "        error_rate = []\n",
    "        \n",
    "        for e in eps:\n",
    "            #print(error_rate)\n",
    "            error_rate.append(np.mean(p_values <= e))\n",
    "\n",
    "        # Plot calibration curve\n",
    "        plt.plot(eps, error_rate, label=f'{scaler_name}')\n",
    "        plt.plot(eps, error_rate, label='Calibration Curve')\n",
    "        plt.plot(eps, eps, '--', label='Ideal')\n",
    "        plt.xlabel('Significance Level (Îµ)')\n",
    "        plt.ylabel('Error Rate')\n",
    "        plt.title('Calibration Curve')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        average_false_p_valueN = calculate_average_false_p_value(model, X_test, y_test, p_values)\n",
    "        print(f\"Average False p-valueN on Test Set for {scaler_name} is {average_false_p_valueN}\")\n",
    "\n",
    "\n",
    "#function responsibe to calcualte the average false p-value.\n",
    "def calculate_average_false_p_value(model, X_test, y_test, p_values):\n",
    "    #print(model)\n",
    "    predictions = model.predict(X_test)\n",
    "    incorrect_predictions = y_test != predictions\n",
    "    \n",
    "    p_values = np.array(p_values)\n",
    "    if len(p_values.shape) == 2:\n",
    "        p_values = np.mean(p_values, axis=0)\n",
    "        #print(\"shape p-valuie after averaging:\",  p_values.shape)\n",
    "    \n",
    "    false_p_values = p_values[incorrect_predictions]\n",
    "    #print(\"false__p_values.size \",false_p_values.size)\n",
    "    if false_p_values.size > 0:\n",
    "        average_false_p_value = np.mean(false_p_values)\n",
    "    else:\n",
    "        average_false_p_value = 0  # or handle as appropriate\n",
    "\n",
    "    return average_false_p_value\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
